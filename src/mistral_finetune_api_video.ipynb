{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urSaPCZoqEmM"
   },
   "source": [
    "# Mistral Fine-tuning API\n",
    "\n",
    "Check out the docs: https://docs.mistral.ai/capabilities/finetuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mistralai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27PpM60GqRvR"
   },
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "In this example, let’s use the ultrachat_200k dataset. We load a chunk of the data into Pandas Dataframes, split the data into training and validation, and save the data into the required jsonl format for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUBRITvX59kC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('./video_script/data/generated_video_conversation.jsonl', lines=True)\n",
    "\n",
    "\n",
    "df_train=df.sample(frac=0.995,random_state=200)\n",
    "df_eval=df.drop(df_train.index)\n",
    "\n",
    "df_train.to_json(\"videos_chunk_train.jsonl\", orient=\"records\", lines=True)\n",
    "df_eval.to_json(\"videos_chunk_eval.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1Pv9zx96CR5",
    "outputId": "e412c365-5887-423a-83b0-1a65b6294ba2"
   },
   "outputs": [],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVfIMwgUJF91"
   },
   "source": [
    "## Reformat dataset\n",
    "If you upload this ultrachat_chunk_train.jsonl to Mistral API, you might encounter an error message “Invalid file format” due to data formatting issues. To reformat the data into the correct format, you can download the reformat_dataset.py script and use it to validate and reformat both the training and evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaLd2n726tAI",
    "outputId": "edb440bc-bc60-40ae-e284-4869812b037e"
   },
   "outputs": [],
   "source": [
    "# download the validation and reformat script\n",
    "!wget https://raw.githubusercontent.com/mistralai/mistral-finetune/main/utils/reformat_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvLwYwfl6vfc",
    "outputId": "bef86874-67fa-4758-a633-1a6e17854b63"
   },
   "outputs": [],
   "source": [
    "# validate and reformat the training data\n",
    "!python reformat_data.py videos_chunk_train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOKqvJMM6kgU"
   },
   "outputs": [],
   "source": [
    "# validate the reformat the eval data\n",
    "!python reformat_data.py videos_chunk_eval.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XO3RVA197Ont",
    "outputId": "19063479-6745-48b8-b694-79896ac2486d"
   },
   "outputs": [],
   "source": [
    "df_train.iloc[104]['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hu_oLukAJect"
   },
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B0UlO1Qa7xi3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai.client import MistralClient\n",
    "\n",
    "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"videos_chunk_train.jsonl\", \"rb\") as f:\n",
    "    videos_chunk_train = client.files.create(file=(\"videos_chunk_train.jsonl\", f))\n",
    "with open(\"videos_chunk_eval.jsonl\", \"rb\") as f:\n",
    "    videos_chunk_eval = client.files.create(file=(\"videos_chunk_eval.jsonl\", f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ChnYoKhoapES"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def pprint(obj):\n",
    "    print(json.dumps(obj.dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIAJdJIc9g2q",
    "outputId": "f5fc042e-8c06-473b-a616-536a0c6dd30c"
   },
   "outputs": [],
   "source": [
    "pprint(videos_chunk_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uA-Xnp1RTmx",
    "outputId": "ac0bdaba-4af1-4f19-a8ab-dd19482b1943"
   },
   "outputs": [],
   "source": [
    "pprint(videos_chunk_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqchXMeXJi6U"
   },
   "source": [
    "## Create a fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9Qk42ADRVKo"
   },
   "outputs": [],
   "source": [
    "from mistralai.models.jobs import TrainingParameters\n",
    "\n",
    "created_jobs = client.jobs.create(\n",
    "    model=\"open-mistral-7b\",#\"mistral-small-latest\", #\"open-mistral-7b\", \n",
    "    training_files=[\"048ac6dd-6636-467f-9326-c46c15d64e0a\"], # videos_chunk_train.id\n",
    "    validation_files=[\"1d871376-0334-4bd5-b431-746f65772254\"], # videos_chunk_eval.id\n",
    "    hyperparameters=TrainingParameters(\n",
    "        training_steps=10,\n",
    "        learning_rate=0.0001,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkZlYvwGanL1",
    "outputId": "396cd040-643b-4296-b026-bb3589df44de"
   },
   "outputs": [],
   "source": [
    "pprint(created_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQXIE2G3c-Ds",
    "outputId": "83b955b3-5666-4b23-abbe-fa4dbe0ec676"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "retrieved_job = client.jobs.retrieve(created_jobs.id)\n",
    "while retrieved_job.status in [\"RUNNING\", \"QUEUED\"]:\n",
    "    retrieved_job = client.jobs.retrieve(created_jobs.id)\n",
    "    pprint(retrieved_job)\n",
    "    print(f\"Job is {retrieved_job.status}, waiting 10 seconds\")\n",
    "    time.sleep(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_x6wRaDtXDzt",
    "outputId": "1efa4948-0ca8-4bc1-85d2-7b72b7e4a931"
   },
   "outputs": [],
   "source": [
    "# List jobs\n",
    "jobs = client.jobs.list()\n",
    "pprint(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve a jobs\n",
    "retrieved_jobs = client.jobs.retrieve(\"68e070f1-b295-41cc-b052-a51c98e9628d\") # \"082bdc92-ba5c-46a9-a937-2cfbf8b21bed\") #created_jobs.id)\n",
    "pprint(retrieved_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OhTWA5uJXHNp",
    "outputId": "a6db9934-a231-4650-d539-17c25e32b8d5"
   },
   "outputs": [],
   "source": [
    "# Retrieve a jobs\n",
    "retrieved_jobs = client.jobs.retrieve(\"f732bd21-cb32-40bd-bdb7-0546e6eed0c6\") #created_jobs.id)\n",
    "pprint(retrieved_jobs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LK-cSS2EJv-e"
   },
   "source": [
    "## Use a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"As a 'Youtube Video Script Writer' for 'GenAI and LLM powered application' influencer\"\n",
    "\n",
    "content = (f\"{role}, \"\n",
    "    f\"your task is to write the script for an engaging video of 5 to 10 minutes (1000 to 2000 words).\"\n",
    "    f\"The script should include a title, the transcript, the author name and a publication date.\"\n",
    "    f\"The video topic is 'Generative AI with LLMs'\"\n",
    "    f\"Here is a  short description: 'Understand the generative AI lifecycle. Describe transformer architecture powering LLMs. Apply training/tuning/inference methods. Hear from researchers on generative AI challenges/opportunities.'\"\n",
    "    f\"Skill level of audience is 'Intermediate', the presenter will be 'Mike Chambers'.\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8vRIB4yXQ5a"
   },
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "chat_response = client.chat(\n",
    "    model=retrieved_jobs.fine_tuned_model,\n",
    "    messages=[ChatMessage(role=\"user\", content=content)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hp0oj5jvX0dP",
    "outputId": "975ed74b-25fe-4e69-c6f8-0a61a69b5a8b"
   },
   "outputs": [],
   "source": [
    "pprint(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_ori = client.chat(\n",
    "        model=\"mistral-small-latest\",\n",
    "        messages=[ChatMessage(role=\"user\", content=content)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_1 = response_ori.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2 = chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_framework = \"\"\"# Evaluation Framework\n",
    "Point attribution should be lowered. In case of 0.5 point, give 0.\n",
    "\n",
    "## Evaluation of the Tone - 12 points\n",
    "\n",
    "The video must follow these writing styles:\n",
    "1. Conciseness: use Short Sentences (less than 25 words per sentence): 1 point\n",
    "2. Use the Present Tense: 1 point\n",
    "3. Use first person: 1 point\n",
    "4. Write in a conversational style: 1 point\n",
    "5. Use more active voice than passive voice: 1 point\n",
    "6. Keep it simple - no jargon employed: 1 point\n",
    "7. Sprinkle in some Humor: 1 point\n",
    "8. Avoid repetition: 1 point\n",
    "9. Avoid conventional messages: 1 point\n",
    "10. Avoid overdoing it or over-sensational with words like “cutting edge”, “revolutionize”: 1 point\n",
    "11. Confident: no words that undermine authority: 1 point\n",
    "12. Energetic and Enthusiastic Tone: 1 point\n",
    "\n",
    "## Evaluation of the Structure and Content: 12 points\n",
    "\n",
    "### Section 1: Video hook and intro: 6 points\n",
    "- Does the script provide enough context for the video to make sense? 1 point\n",
    "- Does the Stakes and payoff are introduce to know why we should watch until the end? 1 point\n",
    "- A curiosity gap is created: What viewers want to know and not all information is given away. 1 point\n",
    "- Leverage input bias: the effort (time, energy, money) that went into the video is showed. 1 point\n",
    "- The video body starts no later than the 20-second mark 1 point\n",
    "- Includes an engaging story or comparison to make the topic relatable 1 point\n",
    "### Section 2: Body, main content, and research: 4 points\n",
    "- Consistent contrast is incorporated to keep things from getting stale 1 point\n",
    "- Good pacing: cycles of high energy and low energy are alternated 1 point\n",
    "    - Each cycle should be 2 - 4 minutes long, with shorter cycles in the beginning and longer cycles at the end 1 point\n",
    "    - The last 20% of long video can be used for slower content, while the beginning of the video is allocated for lighter, faster content 1 point\n",
    "- Critical analysis and personal insights are included 1 point\n",
    "- Practical, real-world applications of the technologies are discussed 1 point\n",
    "- Balanced optimism and realism 1 point\n",
    "### Section 3: CTA (call to action) and conclusion: 2 points\n",
    "- Conclusion leaves a lasting impression by revealing the payoff 1 point\n",
    "- Ends on a high note, either dramatic, wholesome, or funny 1 point\n",
    "\n",
    "## Global score (10): half tone, half structure, and content\n",
    "\"\"\"\n",
    "\n",
    "video_structure = \"\"\"The video must follow this structure :\\n \n",
    "- [Video hook and introduction]\\n\n",
    "- [Body content]\\n\n",
    "- [Conclusion and call to action]\\n\"\"\"\n",
    "\n",
    "writing_tips = \"\"\"The video must follow those writing tips:\n",
    " 1) Use Short Sentences.\n",
    " 2) Use the Present Tense.\n",
    " 3) Use first person\n",
    " 3) Write in a Conversational Style.\n",
    " 4) Use More Active Voice Than Passive Voice.\n",
    " 5) Be Clear and simple: translates jargon into simpler words.\n",
    " 6) Sprinkle in Some Humor.\n",
    " 7) Avoid repetition.\n",
    " 8) Avoid conventional messages and overdoing it with words like “cutting edge”, “revolutionize\n",
    " 9) Be Confident: removes words that undermine authority.\n",
    " 10) Be Concise: makes writing more digestible with fewer than 25 words in a sentence,\n",
    " fewer than 4 sentences per paragraph, and no double descriptions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_comparison = client.chat(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[ChatMessage(role=\"user\", content=f\"\"\"{role},\n",
    "        your task is to refine and rewrite videos transcript to ensure they meet the expected video structure and writing tips.\n",
    "        {video_structure}\n",
    "        {writing_tips}\n",
    "        You are now given two video transcripts.\n",
    "        Read the script carefully and point which one has the most stylistic issues according\n",
    "        to the style guide. Do not rewrite the news articles.\n",
    "        <News_1>{news_1}</News_1>\n",
    "                      \n",
    "        <News_2> {news_2}</News_2>\"\"\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_comparison.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_critique = client.chat(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[ChatMessage(role=\"user\", content=f\"\"\"{role},\n",
    "        your task is to refine and rewrite videos transcript to ensure they meet the high standards of clarity,\n",
    "        precision, and sophistication characteristic of the influencer.\n",
    "        You are now given a evaluation framework.\n",
    "        {evaluation_framework}\n",
    "        Read the transcript carefully and point out all stylistic issues of the given script according\n",
    "        to the framework. Do not rewrite the script. \n",
    "        Finaly grade the script from 1 to 10 based on the level of compliance of the evaluation framework.\n",
    "        \n",
    "        <News>{news_2}</News>\n",
    "        \n",
    "        Critique:\n",
    "        \n",
    "        Grade:\"\"\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_critique.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_critique = client.chat(\n",
    "        model=\"mistral-large-latest\",\n",
    "        messages=[ChatMessage(role=\"user\", content=f\"\"\"{role},\n",
    "        your task is to refine and rewrite videos transcript to ensure they meet the high standards of clarity,\n",
    "        precision, and sophistication characteristic of the influencer.\n",
    "        You are now given a evaluation framework.\n",
    "        {evaluation_framework}\n",
    "        Read the transcript carefully and point out all stylistic issues of the given script according\n",
    "        to the framework. Do not rewrite the script. \n",
    "        Finaly grade the script from 1 to 10 based on the level of compliance of the evaluation framework.\n",
    "        \n",
    "        <News>{news_1}</News>\n",
    "        \n",
    "        Critique:\n",
    "        \n",
    "        Grade:\"\"\")]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_critique.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised video script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "[INTRO]\n",
    "Bonjour, je suis Pierre Bittner animateur d'Applied Ai la chaine youtube l'IA générative et les applications motorisés par les LLM. Je vais vous présenter les résultats de mon hackathon fine tuning mistral.\n",
    "Produire un ton personnel consistant avec les LLMs est un réel défini, qui est encore impossible de solutionner en s'appuyant uniquement avec les techniques de prompt engineering, meme en utilisant les LLM les plus avancés.\n",
    "C'est ce problème que j'ai adressé durant le hackathon. D'ailleurs ce script a été revu par mon modèle fine-tuné.\n",
    "[CONTENT]\n",
    "Mon objectif est de créer une démarche qui permet de produire des modèles fine-tuné qui édite les scripts de vidéos tout en assurant le respect l'unicité du ton, de la structure de la chaine.\n",
    "17 vidéos ont été diffusés sur ma chaine, tous les scripts sont passés par un assistant conversation. Et malgré les nombreuses améliorations sur le processus de production, je n'ai jamais réussi à avoir un résultat satisfaisant en ce qui concerne le respect du style d'écriture ou le respect de la structure qui soit satisfaisant.\n",
    "Au delà de la rédaction de script, se problème se trouve fréquemment par exemple pour la rédaction de rapport d'entreprise ou de nombreux contributeurs participent. Avoir un style cohérent respectant le style de l'entreprise est essentiel et est un travail pénible.\n",
    "Un simple mot comme youtube, influenceur, engageante, clicker, partager suffit pour que le LLM prenne un style totalement inadapté.\n",
    "Le fine-tuning de LLM est mis en avant pour être la solution à ce problème. D'ailleurs c'est un des exemples de la documentation de Mistral. \n",
    "Mais est ce à la porté de tout le monde. C'est ce que l'on va voir.\n",
    "C'était une réelle opportunité de me lancer. Je n'ai que des connaissances théoriques sur le sujet. \n",
    "La plateforme Mistral propose des outils très simple pour réaliser le fine-tuning des modèles qui était accompagné d'un notebook super pratique.\n",
    "Merci également pour les 100€ de crédit qui ont été plus qu'utile puisque plus de 24 millions de token ont été utilisé juste pour ce concours.\n",
    "Faisons un petit zoom sur l'approche que j'ai mis en place pour le cas d'usage.\n",
    "J'ai donc repris l'exemple 3 sur le fine-tuning de ton avec comme exemple des news avec le style The Economist.\n",
    "L'exemple n'était pas opérationnel et se concentrait uniquement sur le format des conversations pour l'apprentissage.\n",
    "J'ai donc reconstruit tout le pipeline pour les news: 1ère étape la génération de news, 2ème étape la génération de critique, 3e étape la génération des conversations.\n",
    "Générer les news avec le style de the economist n'est pas très difficile car celui ci est déjà intégré dans le LLM. Idem pour générer les critiques.\n",
    "La plus grosse difficulté dans cette partie a été finalement de trouver le nombre d'exemple requis pour que le fine-tuning fonctionne. \n",
    "Après de nombreux essais, je suis tombé sur environ 1000 exemples. \n",
    "Le code pour les news est également disponible sur le repo.\n",
    "Une fois ce premier pipeline en place, venait le coeur du défi.\n",
    "Je devais créer un moteur de génération de script de vidéo avec mon style.\n",
    "Autant générer 1000 news de 500 mots, c'est accessibles. Un script de vidéo de 5 à 10', c'est entre 1000 et 2000 mots. On change d'échelle. \n",
    "Les étapes que j'ai suivi pour construire le pipeline de génération sont les suivantes:\n",
    "- j'ai repris tous mes scripts pour les mettre au propres.\n",
    "- Je les ai faites analyser pour identifier leur ton, la structure\n",
    "- J'ai créé les directives de générations de script et surtout un framework suffisamment simple, stable, précis et surtout discriminant pour valider le style et la qualité du contenu.\n",
    "- De nombreux essais ont été nécessaires pour peaufiner les directives et le framework de validation.\n",
    "- Pour générer des idées de vidéo sur l'IA Générative je suis parti des descriptions des cours de deeplearning.ai\n",
    "- Finalement, j'ai réaliser des scripts d'évaluation du résultat des différents modèles.\n",
    "Beaucoup de temps a été passé sur le framework mais pour quel résultat.\n",
    "Malheureusement les benchmarks réalisé sur le model fine tuné versus le model d'origine ou Mistral Large ne montre pas d'amélioration notable au contraire sur la tache de révision des scripts.\n",
    "Après analyse, certaines données de l'entrainement font baisser significativement le score des scripts en particulier pour les scripts déjà bien noté.\n",
    "Les tests unitaires montrent cependant que pour la création de nouveau script, les résultats sont meilleurs.\n",
    "Il était trop tard pour refaire évoluer le pipeline et faire un benchmark spécifique.\n",
    "[CONCLUSION]\n",
    "En tout cas, ce hackathon aura permis de poser de solide base pour un pipeline de fine-tuning de modèle pour le style.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"As a 'Youtube Video Script Writer' for 'GenAI and LLM powered application' influencer\"\n",
    "\n",
    "content = (f\"{role}, \"\n",
    "    f\"your task is to write the script for an engaging video of 2 minutes (500 to 750 words).\"\n",
    "    f\"The video present the result of a hackathon participation on fine-tuning model.\"\n",
    "    f\"The video should follow those rules:\\n\"\n",
    "    f\"{video_structure}\\n\\n\"\n",
    "    f\"{writing_tips}\\n\\n\"\n",
    "    f\"Here is the content of the script that you should use: ####{content}####.\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "chat_response = client.chat(\n",
    "    model=\"ft:mistral-small-latest:c056c2e4:20240628:f732bd21\",\n",
    "    messages=[ChatMessage(role=\"user\", content=content)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"8d94130e17ad4b4bb61c35ca98420875\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1719778074,\n",
      "    \"model\": \"ft:mistral-small-latest:c056c2e4:20240628:f732bd21\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Hello everyone, I'm Pierre Bittner, host of Applied Ai, the YouTube channel dedicated to generative AI and applications powered by LLMs. Today, I'll be sharing my findings from the Mistral Fine Tuning Hackathon.\\n\\nProducing a personal tone consistent with LLMs is a real challenge that cannot be solved solely by relying on prompt engineering techniques, even when using the most advanced LLMs. This is the problem I addressed during the hackathon. In fact, this script has been reviewed by my fine-tuned model.\\n\\nMy goal is to create an approach that allows for the production of fine-tuned models that edit video scripts while ensuring respect for the unique tone and structure of the channel. Seventeen videos have been posted on my channel, and all the scripts have gone through a conversation assistant. Despite numerous improvements to the production process, I've never been able to achieve a satisfactory result in terms of maintaining the writing style or structure.\\n\\nThis issue goes beyond just script writing. It's also frequently encountered in the writing of company reports where multiple contributors are involved. Having a coherent style that respects the company's style is essential and is a laborious task.\\n\\nA simple word like \\\"YouTube,\\\" \\\"influencer,\\\" \\\"engaging,\\\" \\\"clicker,\\\" or \\\"share\\\" is enough for the LLM to take on a completely inappropriate style. Fine-tuning LLMs is being touted as the solution to this problem, and it's one of the examples in Mistral's documentation. But is it within everyone's reach? Let's find out.\\n\\nParticipating in this hackathon was a great opportunity for me. I only have theoretical knowledge on the subject. Mistral's platform offers very simple tools for fine-tuning models, which is accompanied by a super practical notebook. I'm also grateful for the 100\\u20ac credit, which was more than useful since over 24 million tokens were used just for this competition.\\n\\nLet's take a closer look at the approach I took for this use case. I used example 3 for fine-tuning the tone with news written in The Economist's style. The example was not operational and only focused on the format of conversations for learning.\\n\\nI rebuilt the entire pipeline for the news: the first step was generating news, the second step was generating critiques, and the third step was generating conversations. Generating news in The Economist's style is not very difficult because it's already integrated into the LLM. Generating critiques was also straightforward. The biggest challenge in this part was finding the number of examples required for the fine-tuning to work.\\n\\nAfter numerous trials, I found that about 1,000 examples were needed. The code for the news is also available on the repo. Once this first pipeline was in place, I faced the main challenge. I had to create a script generation engine with my style.\\n\\nGenerating 1,000 news articles of 500 words each is accessible. A video script of 5 to 10 minutes, which is between 1,000 and 2,000 words, is a different scale altogether. The steps I followed to build the script generation pipeline are as follows:\\n\\n- I cleaned up all my scripts.\\n- I had them analyzed to identify their tone and structure.\\n- I created script generation guidelines and, more importantly, a framework that was simple, stable, precise, and discriminating enough to validate the style and content quality.\\n- Numerous trials were needed to refine the guidelines and the validation framework.\\n- To generate video ideas about generative AI, I started with the course descriptions from deeplearning.ai.\\n- Finally, I created evaluation scripts for the results of the different models.\\n\\nUnfortunately, the benchmarks I conducted on the fine-tuned model versus the original model or Mistral Large did not show any significant improvement in script revision. After analysis, certain training data significantly reduced the script scores, particularly for scripts that were already well-rated.\\n\\nHowever, unit tests showed that for creating new scripts, the results were better. It was too late to evolve the pipeline and conduct a specific benchmark.\\n\\nIn conclusion, this hackathon laid the groundwork for a solid fine-tuning model pipeline for style.\",\n",
      "                \"name\": null,\n",
      "                \"tool_calls\": null,\n",
      "                \"tool_call_id\": null\n",
      "            },\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 1447,\n",
      "        \"total_tokens\": 2386,\n",
      "        \"completion_tokens\": 939\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pprint(chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello everyone, I\\'m Pierre Bittner, host of Applied Ai, the YouTube channel dedicated to generative AI and applications powered by LLMs. Today, I\\'ll be sharing my findings from the Mistral Fine Tuning Hackathon.\\n\\nProducing a personal tone consistent with LLMs is a real challenge that cannot be solved solely by relying on prompt engineering techniques, even when using the most advanced LLMs. This is the problem I addressed during the hackathon. In fact, this script has been reviewed by my fine-tuned model.\\n\\nMy goal is to create an approach that allows for the production of fine-tuned models that edit video scripts while ensuring respect for the unique tone and structure of the channel. Seventeen videos have been posted on my channel, and all the scripts have gone through a conversation assistant. Despite numerous improvements to the production process, I\\'ve never been able to achieve a satisfactory result in terms of maintaining the writing style or structure.\\n\\nThis issue goes beyond just script writing. It\\'s also frequently encountered in the writing of company reports where multiple contributors are involved. Having a coherent style that respects the company\\'s style is essential and is a laborious task.\\n\\nA simple word like \"YouTube,\" \"influencer,\" \"engaging,\" \"clicker,\" or \"share\" is enough for the LLM to take on a completely inappropriate style. Fine-tuning LLMs is being touted as the solution to this problem, and it\\'s one of the examples in Mistral\\'s documentation. But is it within everyone\\'s reach? Let\\'s find out.\\n\\nParticipating in this hackathon was a great opportunity for me. I only have theoretical knowledge on the subject. Mistral\\'s platform offers very simple tools for fine-tuning models, which is accompanied by a super practical notebook. I\\'m also grateful for the 100€ credit, which was more than useful since over 24 million tokens were used just for this competition.\\n\\nLet\\'s take a closer look at the approach I took for this use case. I used example 3 for fine-tuning the tone with news written in The Economist\\'s style. The example was not operational and only focused on the format of conversations for learning.\\n\\nI rebuilt the entire pipeline for the news: the first step was generating news, the second step was generating critiques, and the third step was generating conversations. Generating news in The Economist\\'s style is not very difficult because it\\'s already integrated into the LLM. Generating critiques was also straightforward. The biggest challenge in this part was finding the number of examples required for the fine-tuning to work.\\n\\nAfter numerous trials, I found that about 1,000 examples were needed. The code for the news is also available on the repo. Once this first pipeline was in place, I faced the main challenge. I had to create a script generation engine with my style.\\n\\nGenerating 1,000 news articles of 500 words each is accessible. A video script of 5 to 10 minutes, which is between 1,000 and 2,000 words, is a different scale altogether. The steps I followed to build the script generation pipeline are as follows:\\n\\n- I cleaned up all my scripts.\\n- I had them analyzed to identify their tone and structure.\\n- I created script generation guidelines and, more importantly, a framework that was simple, stable, precise, and discriminating enough to validate the style and content quality.\\n- Numerous trials were needed to refine the guidelines and the validation framework.\\n- To generate video ideas about generative AI, I started with the course descriptions from deeplearning.ai.\\n- Finally, I created evaluation scripts for the results of the different models.\\n\\nUnfortunately, the benchmarks I conducted on the fine-tuned model versus the original model or Mistral Large did not show any significant improvement in script revision. After analysis, certain training data significantly reduced the script scores, particularly for scripts that were already well-rated.\\n\\nHowever, unit tests showed that for creating new scripts, the results were better. It was too late to evolve the pipeline and conduct a specific benchmark.\\n\\nIn conclusion, this hackathon laid the groundwork for a solid fine-tuning model pipeline for style.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xA68sx-J1Jj"
   },
   "source": [
    "## Integration with Weights and Biases\n",
    "We can also offer support for integration with Weights & Biases (W&B) to monitor and track various metrics and statistics associated with our fine-tuning jobs. To enable integration with W&B, you will need to create an account with W&B and add your W&B information in the “integrations” section in the job creation request:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7ZYQzZ7YcYb",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from mistralai.models.jobs import WandbIntegrationIn\n",
    "\n",
    "WANDB_API_KEY = \"XXX\"\n",
    "\n",
    "created_jobs = client.jobs.create(\n",
    "    model=\"open-mistral-7b\",\n",
    "    training_files=[ultrachat_chunk_train.id],\n",
    "    validation_files=[ultrachat_chunk_eval.id],\n",
    "    hyperparameters=TrainingParameters(\n",
    "        training_steps=100,\n",
    "        learning_rate=0.0001,\n",
    "    ),\n",
    "    integrations=[\n",
    "        WandbIntegrationIn(\n",
    "            project=\"test_ft_api\",\n",
    "            run_name=\"test\",\n",
    "            api_key=WANDB_API_KEY,\n",
    "        ).dict()\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Poetry Mistral)",
   "language": "python",
   "name": "mistral-fine-tuning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
