{"video": {"title": "Mastering Function-Calling and Data Extraction with LLMs", "transcript": "Hi there, I'm Jiantao Jiao and today we're diving into the exciting world of Function-Calling and Data Extraction with Language Learning Models, or LLMs. If you're familiar with LLMs and have some basic Python knowledge, you're in the right place! \n\nFirst up, let's talk about function-calling. It's a game-changer for expanding the capabilities of LLMs and agent applications. With function-calling, you can extend LLMs with custom functionality, enabling them to form calls to external functions. Pretty cool, right? \n\nNext, we'll explore data extraction. Imagine being able to extract structured data from natural language inputs. That's exactly what we'll be doing! This makes real-world data usable for analysis, opening up a whole new world of possibilities. \n\nBut that's not all. We'll also build an end-to-end application that processes customer service transcripts using LLMs. This is a practical, hands-on way to see how these concepts work in the real world. \n\nAnd guess what? We're partnering with Nexusflow to bring you this content. They're a leading name in the field, so you know you're learning from the best. \n\nSo, are you ready to revolutionize your understanding of LLMs? Let's get started! Remember, keep practicing and don't forget to have fun. \n\nThanks for watching. Don't forget to like, share, and subscribe for more exciting content. See you in the next video!", "author": "Jiantao Jiao, Venkat Srinivasan", "publication_date": "2022-01-01"}}
{"video": {"title": "Harnessing the Power of AI Agents with LangGraph and Tavily's Agentic Search", "transcript": "Hello, AI enthusiasts! I'm Harrison Chase, the founder of LangChain, and joining me is Rotem Weiss, the founder of Tavily. Today, we're excited to show you how to build agentic AI workflows using LangGraph's LangGraph and Tavily's agentic search. \n\nFirst, let's dive into LangGraph's components and how they enable the development, debugging, and maintenance of AI agents. LangGraph is an open-source framework that makes it easy to create more controllable agents. \n\nNow, let's take it up a notch by integrating Tavily's agentic search capabilities. This will enhance your agent's knowledge and performance, making it more efficient and effective. \n\nDon't worry if you're not an expert in Python. This course is designed for those with intermediate Python knowledge. We'll guide you through each step, making sure you understand the concepts and how to apply them. \n\nAnd the best part? You'll be learning directly from us, the founders of LangChain and Tavily. We'll share our insights, tips, and tricks to help you make the most of these powerful tools. \n\nSo, are you ready to revolutionize the way you build AI agents? Let's get started! Remember, practice is key. The more you work with these tools, the better you'll become. \n\nThanks for watching. Don't forget to like, share, and subscribe for more exciting content. See you in the next video!", "author": "Harrison Chase, Rotem Weiss", "publication_date": "2023-03-15"}}

{"video": {"title": "On-Device AI: A New Era of Edge Computing", "transcript": "Hi there, I'm Krishna Sridhar, and today we're diving into the exciting world of On-Device AI! \n\nImagine having AI right in your pocket, on your smartphone or other edge devices. That's the power of On-Device AI. It leverages the local compute power of your device for faster and more secure inference. No more waiting for cloud processing! \n\nFirst, let's talk about model conversion. If you're familiar with Python, PyTorch, or TensorFlow, you're in luck! We'll learn how to convert these models for device compatibility. It's like translating AI language for your device to understand. \n\nNext up, quantization. It's a fancy word for reducing model size while achieving performance gains. Think of it like packing a suitcase efficiently - more space, less weight, but still having everything you need. \n\nNow, let's get our hands dirty with device integration. We'll explore runtime dependencies and how GPU, NPU, and CPU compute unit utilization affect performance. It's like understanding how different engines in a car work together to give you a smooth ride. \n\nAnd guess what? We're partnering with Qualcomm to bring you this exciting journey. So, buckle up and get ready to revolutionize the way you think about AI! \n\nRemember, keep practicing, keep exploring, and don't forget to hit that like button, share this video, and subscribe for more exciting content. Until next time, this is Krishna Sridhar, signing off.", "author": "Krishna Sridhar", "publication_date": "2023-03-15"}}
{"video": {"title": "Unleashing the Power of Multi AI Agent Systems with crewAI", "transcript": "Hi there, I'm Jo\u00e3o Moura, and today we're diving into the exciting world of Multi AI Agent Systems with crewAI! \n\nAre you familiar with prompt engineering and basic coding? Looking to incorporate Large Language Models (LLMs) into your professional work? Then you're in the right place! \n\nImagine automating business workflows with not just one, but a team of AI agents. With crewAI, an open-source library, you can exceed the performance of prompting a single LLM by designing and prompting a team of AI agents through natural language. \n\nSounds complex? Don't worry, it's simpler than you think. Let's break it down. \n\nFirst, let's understand what crewAI is. It's an open-source library that allows you to automate repeatable, multi-step tasks. Think of it like tailoring a resume to a job description, or automating business processes that are typically done by a group of people, like event planning. \n\nSo, how does it work? By creating a team of AI agents, you can define a specific role, goal, and backstory for each agent. This breaks down complex multi-step tasks and assigns them to agents that are customized to perform those tasks. \n\nLet's take an example. Suppose you're planning an event. You can create an AI agent responsible for venue selection, another for catering, and another for guest list management. Each agent has a specific role and works together to achieve the common goal of a successful event. \n\nNow, isn't that amazing? With crewAI, you're not just automating tasks, you're creating a team of AI agents that work together to achieve your goals. \n\nSo, are you ready to start building your own team of AI agents? Join me in this journey as we explore more about crewAI and how it can revolutionize your workflows. \n\nRemember, if you have any questions or need further clarification, feel free to leave a comment below. And don't forget to like, share, and subscribe for more exciting content on AI and automation. \n\nUntil next time, keep learning and keep innovating!", "author": "Jo\u00e3o Moura", "publication_date": "2023-03-15"}}
{"video": {"title": "Building Autonomous Agents with LlamaIndex: A Beginner's Guide", "transcript": "Hi there, I'm Jerry Liu, and today we're diving into the exciting world of autonomous agents! We'll be using LlamaIndex to build an Agentic RAG system that can intelligently navigate and analyze your data. \n\nFirst things first, let's make sure you've got the basics of Python under your belt. If you're new to Python, don't worry! We'll keep things simple and easy to follow. \n\nNow, let's get started by building an agent that can reason over your documents and answer complex questions. Sounds cool, right? Imagine asking your agent about the latest sales report, and it gives you a perfect summary with all the key points. \n\nNext, we'll build a router agent that can help you with Q&A and summarization tasks. And guess what? We'll even extend it to handle passing arguments to this agent. This means you can customize your agent to suit your specific needs. \n\nOnce we've got that down, we'll design a research agent that handles multi-documents. This is where things get really interesting. Your agent will be able to process and analyze multiple documents at once, making it a powerful tool for research and data analysis. \n\nAnd finally, we'll look at different ways to debug and control this agent. After all, even the smartest agents can sometimes make mistakes. But don't worry, I'll show you how to troubleshoot and fix any issues that come up. \n\nBy the end of this video, you'll have gained valuable skills in guiding agent reasoning and debugging. And the best part? You'll be able to build your own autonomous agents that can intelligently navigate and analyze your data. \n\nSo, are you ready to revolutionize the way you interact with your data? Let's get started! \n\nRemember, if you have any questions or need further clarification, don't hesitate to leave a comment below. And don't forget to like, share, and subscribe for more exciting content. See you in the next video!", "author": "Jerry Liu", "publication_date": "2023-03-15"}}
{"video": {"title": "Mastering Machine Learning in Production: A Comprehensive Guide", "transcript": "Hi there, I'm Andrew Ng, and today we're diving into the exciting world of Machine Learning in Production! \n\nFirst, let's talk about scoping. It's like planning a road trip - you need to know where you're going before you start the engine. We'll discuss how to define your ML project's objectives and set clear, measurable goals. \n\nNext, we'll delve into data. It's the fuel for our ML engine. We'll explore data collection, preprocessing, and how to handle missing or incorrect data. \n\nThen, we'll get our hands dirty with modeling. We'll cover selecting the right algorithm, training your model, and evaluating its performance. \n\nOnce we've built our model, it's time for deployment. We'll walk through integrating your model into your existing systems, monitoring its performance, and handling any issues that arise. \n\nFinally, we'll discuss continuous improvement. Just like a well-oiled machine, our ML system needs regular maintenance and updates to stay in top shape. \n\nRemember, building an ML production system is a journey, not a destination. So, buckle up, and let's get started! \n\nStay tuned for more insights in our next video. Don't forget to like, share, and subscribe for more exciting content on GenAI and LLM powered applications. Until next time, keep learning!", "author": "Andrew Ng", "publication_date": "2022-01-01"}}
{"video": {"title": "Prototyping Your ML Production System", "transcript": "Hello, ML enthusiasts! Andrew Ng here, and today we're talking about prototyping your ML production system. \n\nThink of prototyping as building a mini version of your ML system. It's a chance to test your ideas, spot any issues, and make improvements before you go live. \n\nWe'll discuss how to create a prototype, test it, and iterate based on feedback. We'll also cover common pitfalls and how to avoid them. \n\nRemember, prototyping is all about learning and improving. So, don't be afraid to make mistakes - that's how we grow! \n\nJoin me in our next video as we continue our journey into ML production systems. Don't forget to like, share, and subscribe for more insightful content. Until next time, keep exploring!", "author": "Andrew Ng", "publication_date": "2022-01-08"}}
{"video": {"title": "Deploying Your ML Model: A Step-by-Step Guide", "transcript": "Hey there, it's Andrew Ng, and today we're discussing how to deploy your ML model. \n\nDeployment is like launching a rocket - it's exciting, but it needs careful planning and execution. We'll cover how to prepare your model for deployment, integrate it into your existing systems, and monitor its performance. \n\nWe'll also discuss how to handle any issues that arise and ensure your model continues to deliver accurate predictions. \n\nRemember, deployment is just the beginning. Your ML model needs regular care and attention to stay in top shape. \n\nStay tuned for more insights in our next video. Don't forget to like, share, and subscribe for more exciting content on GenAI and LLM powered applications. Until next time, keep innovating!", "author": "Andrew Ng", "publication_date": "2022-01-15"}}
{"video": {"title": "Continuous Improvement in ML Production Systems", "transcript": "Hi, I'm Andrew Ng, and today we're talking about continuous improvement in ML production systems. \n\nJust like a well-oiled machine, your ML system needs regular maintenance and updates to stay in top shape. We'll discuss how to monitor your model's performance, identify areas for improvement, and implement updates. \n\nWe'll also cover how to test these updates and ensure they're improving your model's performance. \n\nRemember, continuous improvement is a journey, not a destination. So, keep learning, keep improving, and keep pushing the boundaries of what's possible! \n\nJoin me in our next video as we continue our journey into ML production systems. Don't forget to like, share, and subscribe for more insightful content. Until next time, keep learning!", "author": "Andrew Ng", "publication_date": "2022-01-22"}}
{"video": {"title": "Mastering Quantization: A Deep Dive into Advanced Techniques", "transcript": "Hi there, I'm Marc Sun, and today we're diving deep into the world of quantization. If you've taken our Quantization Fundamentals course, you're in the perfect spot to build on that knowledge. \n\nFirst up, we're exploring different variants of Linear Quantization. We'll compare symmetric and asymmetric modes, and discuss the pros and cons of each. We'll also look at different granularities, like per tensor, per channel, and per group quantization. \n\nNext, we're getting hands-on. We'll build a general-purpose quantizer in Pytorch. This bad boy can quantize the dense layers of any open source model, giving you up to 4x compression on dense layers. Pretty cool, right? \n\nBut we're not stopping there. We're also going to implement weights packing. This technique lets us pack four 2-bit weights into a single 8-bit integer. It's like a magic trick for your models. \n\nAnd guess what? We're partnering with Hugging Face for this adventure. So you know you're getting top-notch resources and guidance. \n\nRemember, this course is designed for an intermediate skill level. So if you're new to quantization, I recommend checking out our Quantization Fundamentals course first. \n\nThat's it for today's preview. If you're ready to level up your quantization skills, let's get started. And don't forget to like, share, and subscribe for more great content. See you in the course!", "author": "Marc Sun, Younes Belkada", "publication_date": "2022-01-01"}}
{"video": {"title": "Linear Quantization: Symmetric vs. Asymmetric Mode", "transcript": "Hey there, I'm Marc Sun, and welcome back to our series on advanced quantization techniques. Today, we're comparing symmetric and asymmetric modes in Linear Quantization. \n\nFirst, let's talk about symmetric mode. In this mode, the zero point is always zero. This means that both positive and negative numbers are represented equally. \n\nOn the other hand, asymmetric mode allows for a non-zero zero point. This can be beneficial when your data is not centered around zero. \n\nWe'll dive into the math behind these modes and discuss when to use each one. By the end of this video, you'll be a pro at choosing the right mode for your quantization needs. \n\nRemember, this course is brought to you in partnership with Hugging Face. So you're getting the best resources and guidance out there. \n\nThat's it for today's preview. If you're ready to master symmetric and asymmetric modes, let's get started. And don't forget to like, share, and subscribe for more great content. See you in the course!", "author": "Marc Sun, Younes Belkada", "publication_date": "2022-01-08"}}
{"video": {"title": "Granularity in Linear Quantization: Per Tensor, Per Channel, and Per Group", "transcript": "Hi there, I'm Marc Sun, and today we're talking about granularity in Linear Quantization. We'll cover per tensor, per channel, and per group quantization. \n\nFirst up, per tensor quantization. This is the simplest form of quantization, where all weights in a tensor share the same quantization parameters. \n\nNext, we'll discuss per channel quantization. This is where each channel in a weight tensor has its own set of quantization parameters. \n\nFinally, we'll dive into per group quantization. This is a more advanced technique where weights are divided into groups, and each group has its own quantization parameters. \n\nWe'll discuss the benefits and drawbacks of each method, and when to use each one. By the end of this video, you'll be a granularity guru. \n\nRemember, this course is designed for an intermediate skill level, and it's brought to you in partnership with Hugging Face. \n\nThat's it for today's preview. If you're ready to master granularity in Linear Quantization, let's get started. And don't forget to like, share, and subscribe for more great content. See you in the course!", "author": "Marc Sun, Younes Belkada", "publication_date": "2022-01-15"}}
{"video": {"title": "Building a General-Purpose Quantizer in Pytorch", "transcript": "Hey there, I'm Marc Sun, and today we're getting hands-on. We're building a general-purpose quantizer in Pytorch. \n\nFirst, we'll discuss the theory behind quantization in Pytorch. Then, we'll dive into the code. We'll walk through each step of building a quantizer that can quantize the dense layers of any open source model. \n\nBy the end of this video, you'll have a quantizer that can give you up to 4x compression on dense layers. Not too shabby, right? \n\nRemember, this course is brought to you in partnership with Hugging Face. So you're getting top-notch resources and guidance. \n\nThat's it for today's preview. If you're ready to build your own quantizer, let's get started. And don't forget to like, share, and subscribe for more great content. See you in the course!", "author": "Marc Sun, Younes Belkada", "publication_date": "2022-01-22"}}
{"video": {"title": "Implementing Weights Packing: Pack Four 2-bit Weights into a Single 8-bit Integer", "transcript": "Hi there, I'm Marc Sun, and today we're talking about weights packing. Specifically, we're going to learn how to pack four 2-bit weights into a single 8-bit integer. \n\nFirst, we'll discuss the theory behind weights packing. Then, we'll dive into the code. We'll walk through each step of implementing weights packing in your models. \n\nBy the end of this video, you'll be a weights packing pro. You'll know how to maximize your model's efficiency without sacrificing performance. \n\nRemember, this course is designed for an intermediate skill level, and it's brought to you in partnership with Hugging Face. \n\nThat's it for today's preview. If you're ready to master weights packing, let's get started. And don't forget to like, share, and subscribe for more great content. See you in the course!", "author": "Marc Sun, Younes Belkada", "publication_date": "2022-01-29"}}

{"video": {"title": "Quantization 101: Shrinking Models with Hugging Face and Quanto", "transcript": "Hi there, I'm Younes Belkada and today, along with my colleague Marc Sun, we're going to dive into the world of model quantization using Hugging Face Transformers library and the Quanto library. \n\nNow, you might be wondering, what is quantization? Well, it's a simple yet effective method for compressing models, making them smaller and faster. \n\nFirst, we'll learn about linear quantization, the most common method used in quantization. It's like shrinking your model's clothes to fit into a smaller suitcase. \n\nThen, we'll get our hands dirty and practice quantizing open source multimodal and language models. Don't worry, it's easier than it sounds. \n\nAnd the best part? We're partnering with Hugging Face to bring you this content. So, you'll be learning from the best in the industry. \n\nBy the end of this video, you'll be a pro at compressing models with Hugging Face Transformers library and the Quanto library. \n\nSo, are you ready to shrink some models? Let's get started! \n\nRemember, if you have any questions, leave them in the comments below. And don't forget to like, share, and subscribe for more exciting content. \n\nUntil next time, keep learning and keep growing.", "author": "Younes Belkada, Marc Sun", "publication_date": "2023-03-15"}}
{"video": {"title": "Preprocessing Unstructured Data for LLM Applications: A Beginner's Guide", "transcript": "Hi there, I'm Matt Robinson, and today we're diving into the exciting world of preprocessing unstructured data for LLM applications. \n\nFirst things first, what does that even mean? Well, if you've been working on improving your RAG system to retrieve diverse data formats, you're in the right place. \n\nWe're going to learn how to extract and normalize content from a wide variety of document types, such as PDFs, PowerPoints, Word, and HTML files. But that's not all! We'll also explore how to preprocess tables and images to expand the information accessible to your LLM. \n\nNext, we'll enrich our content with metadata. This little trick will enhance our retrieval augmented generation (RAG) results and support more nuanced search capabilities. Pretty cool, right? \n\nBut wait, there's more! We'll also explore document image analysis techniques like layout detection and vision and table transformers. And guess what? We'll learn how to apply these methods to preprocess PDFs, images, and tables. \n\nSo, are you ready to take your LLM applications to the next level? Let's get started! \n\nRemember, practice makes perfect. So, don't be afraid to try new things and make mistakes. That's how we learn. And if you have any questions, feel free to leave them in the comments below. I'm always here to help. \n\nThanks for watching, and don't forget to like, share, and subscribe for more exciting content. See you in the next video!", "author": "Matt Robinson", "publication_date": "2023-03-15"}}
{"video": {"title": "Red Teaming for Safer LLM Applications: A Beginner's Guide", "transcript": "Hi there, I'm Matteo Dora and today we're diving into the world of red teaming for large language model applications, or LLMs for short. \n\nFirst things first, what is red teaming? It's a technique borrowed from cybersecurity where you intentionally challenge your system to identify vulnerabilities and weaknesses. \n\nIn the context of LLM applications, red teaming helps us ensure the safety and reliability of our apps. It's all about finding potential issues before they become real problems. \n\nNow, you might be wondering, 'Why do I need to know this?' Well, if you're building LLM applications, it's crucial to make sure they're robust and secure. And that's where red teaming comes in. \n\nDon't worry if you're new to this. This course is beginner-friendly, although having some basic Python knowledge will help you get the most out of it. \n\nWe'll start by learning how to identify and evaluate vulnerabilities in LLM applications. Then, we'll apply red teaming techniques to address these vulnerabilities. \n\nAnd here's the best part: we'll be using an open source library from our technology partner, Giskard, to help automate these methods. \n\nSo, are you ready to make your LLM applications safer and more reliable? Let's get started with red teaming! \n\nRemember, the key to successful red teaming is constant learning and improvement. So, keep exploring, keep learning, and don't forget to have fun along the way. \n\nThanks for watching, and stay tuned for more tips and tricks on building better LLM applications. \n\nDon't forget to like, share, and subscribe for more content like this. See you in the next video!", "author": "Matteo Dora, Luca Martial", "publication_date": "2023-03-15"}}

{"video": {"title": "Mastering Knowledge Graphs for RAG with Neo4j and LangChain", "transcript": "Hi there, I'm Andreas Kollegger, and today we're diving into the exciting world of Knowledge Graphs for Retrieval Augmented Generation, or RAG for short. If you're familiar with LangChain or have taken our short course 'LangChain: Chat with Your Data', you're in the perfect spot to learn how to build and use knowledge graph systems to supercharge your RAG applications. \n\nFirst off, what's a knowledge graph? Well, imagine a giant web of data points, all connected by different relationships. That's a knowledge graph. And today, we're using Neo4j, a powerful graph database, to manage and retrieve this data. \n\nNeo4j uses a query language called Cypher. It's like SQL, but for graphs. With Cypher, we can write queries that find and format text data, providing more relevant context to our Language Learning Models (LLMs) for RAG. \n\nNow, let's get our hands dirty and build a question-answering system using Neo4j and LangChain. We'll start by creating a knowledge graph from structured text documents. Then, we'll write a Cypher query to find the most relevant data. Finally, we'll use LangChain to generate a response based on the retrieved data. \n\nAnd that's it! You've just built a question-answering system powered by a knowledge graph. With this new skill, you can improve the relevance and accuracy of your RAG applications. \n\nRemember, practice makes perfect. So, keep experimenting with different knowledge graphs and queries. And if you're stuck, Neo4j has a great community and resources to help you out. \n\nThanks for watching, and happy coding! Don't forget to like, share, and subscribe for more exciting content. See you in the next video.", "author": "Andreas Kollegger", "publication_date": "2023-04-01"}}
{"video": {"title": "Unleashing the Power of AI with Hugging Face Open Source Models", "transcript": "Hi there, I'm Maria, and today we're diving into the exciting world of AI, made easy with Hugging Face's open-source models! \n\nFirst off, let's head over to the Hugging Face Hub. It's like a candy store, but for AI models. You'll find a vast array of models, all open source and ready to use. \n\nNow, how do you choose the right model? It's simple. You can filter them based on the task you need, their rankings, and even their memory requirements. \n\nOnce you've picked your model, the magic begins. With just a few lines of code, using the transformers library, you can perform text, audio, image, and even multimodal tasks. It's like having your own AI assistant! \n\nBut wait, there's more. Want to share your AI apps with the world? No problem. With a user-friendly interface provided by Gradio and Hugging Face Spaces, you can easily share your apps and even run them on the cloud. \n\nSo, are you ready to revolutionize your projects with AI? Remember, you don't need to be an expert to start. With Hugging Face, AI is for everyone. \n\nStay tuned for more tips and tricks on our channel. And don't forget to like, share, and subscribe! \n\nUntil next time, keep exploring and innovating.", "author": "Maria Khalusova, Marc Sun, Younes Belkada", "publication_date": "2022-01-01"}}


{"video": {"title": "Mastering Generative AI with Language Models", "transcript": "Hi there, I'm Antje Barth, and today we're diving into the exciting world of Generative AI with Language Models, or LLMs! \n\nFirst, let's talk about what Generative AI is and how it works. Generative AI is a type of artificial intelligence that can create new content, such as images, music, or text, by learning patterns from existing data. LLMs are a specific type of Generative AI that focus on generating text. \n\nAt the heart of LLMs is the transformer architecture, which allows the model to process input text in parallel and generate output text one word at a time. This architecture has revolutionized the field of natural language processing and enabled the creation of powerful LLMs like GPT-3. \n\nNow, let's talk about how to train, tune, and deploy LLMs. Training an LLM involves feeding it large amounts of text data and using techniques like backpropagation to adjust the model's weights and improve its accuracy. Tuning involves adjusting the model's hyperparameters to optimize its performance on specific tasks. And deployment involves integrating the model into a larger system, such as a chatbot or content generation tool. \n\nBut what about the challenges and opportunities of Generative AI? We'll hear from researchers in the field about the ethical considerations of creating AI that can generate realistic text, as well as the potential applications of LLMs in fields like healthcare, finance, and entertainment. \n\nBy the end of this video series, you'll have a foundational knowledge of Generative AI with LLMs, practical skills for training and deploying LLMs, and a functional understanding of how this technology is being used to create value in the real world. You'll also hear from expert AWS AI practitioners who actively build and deploy AI in business use-cases today. \n\nSo, are you ready to dive into the world of Generative AI with LLMs? Let's get started!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-03-01"}}
{"video": {"title": "Transformer Architecture: The Key to LLMs", "transcript": "Hey there, Chris Fregly here, and today we're going to take a closer look at the transformer architecture that powers LLMs. \n\nThe transformer architecture was introduced in a paper by Vaswani et al. in 2017 and has since become the go-to architecture for natural language processing tasks. It allows LLMs to process input text in parallel, rather than sequentially, which greatly improves the model's efficiency and accuracy. \n\nBut how exactly does the transformer architecture work? At a high level, it consists of encoder and decoder layers that use self-attention mechanisms to weigh the importance of different words in the input text. This allows the model to understand the context of each word and generate more accurate output text. \n\nWe'll also talk about some of the challenges of training LLMs with the transformer architecture, such as the need for large amounts of data and computational resources. And we'll discuss some techniques for addressing these challenges, such as transfer learning and fine-tuning. \n\nBy the end of this video, you'll have a solid understanding of the transformer architecture and how it enables LLMs to generate high-quality text. So let's dive in!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-03-08"}}
{"video": {"title": "Training and Tuning LLMs for Optimal Performance", "transcript": "Hi, I'm Shelbee Eigenbrode, and today we're going to talk about how to train and tune LLMs for optimal performance. \n\nTraining an LLM involves feeding it large amounts of text data and using techniques like backpropagation to adjust the model's weights and improve its accuracy. But there are many other factors to consider when training an LLM, such as the choice of loss function, the learning rate, and the batch size. \n\nWe'll also talk about how to fine-tune an LLM for specific tasks, such as text classification or named entity recognition. Fine-tuning involves adjusting the model's weights on a smaller dataset that is relevant to the task at hand. This can greatly improve the model's performance on the task without requiring as much data or computational resources as training from scratch. \n\nBut how do you know when your LLM is performing optimally? We'll discuss some metrics for evaluating LLM performance, such as perplexity and BLEU score, and how to interpret these metrics to make informed decisions about your model. \n\nBy the end of this video, you'll have a solid understanding of how to train and tune LLMs for optimal performance on a variety of tasks. So let's get started!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-03-15"}}
{"video": {"title": "Deploying LLMs in the Real World", "transcript": "Hey there, Mike Chambers here, and today we're going to talk about how to deploy LLMs in the real world. \n\nDeploying an LLM involves integrating it into a larger system, such as a chatbot or content generation tool. But there are many considerations to keep in mind when deploying an LLM, such as the model's latency, scalability, and security. \n\nWe'll talk about some best practices for deploying LLMs, such as using containerization and orchestration tools like Docker and Kubernetes, and implementing authentication and authorization measures to protect your model from unauthorized access. \n\nWe'll also discuss some real-world applications of LLMs, such as generating personalized product descriptions for e-commerce sites or creating realistic dialogue for video games. And we'll hear from some AWS AI practitioners who are actively building and deploying LLMs in business use-cases today. \n\nBy the end of this video, you'll have a solid understanding of how to deploy LLMs in the real world and some inspiration for how you can use this technology to create value in your own projects. So let's dive in!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-03-22"}}
{"video": {"title": "The Ethics of Generative AI with LLMs", "transcript": "Hi, I'm Antje Barth, and today we're going to talk about the ethics of Generative AI with LLMs. \n\nAs LLMs become more powerful and capable of generating increasingly realistic text, there are many ethical considerations to keep in mind. For example, how do we ensure that LLMs are not used to spread misinformation or propaganda? And how do we address the potential for LLMs to perpetuate biases that exist in the training data? \n\nWe'll also talk about some of the potential benefits of LLMs, such as their ability to generate creative content and assist with language translation. And we'll discuss some potential applications of LLMs in fields like healthcare, finance, and entertainment. \n\nBy the end of this video, you'll have a better understanding of the ethical considerations surrounding LLMs and some ideas for how to use this technology responsibly. So let's get started!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-03-29"}}
{"video": {"title": "The Future of Generative AI with LLMs", "transcript": "Hey there, Chris Fregly here, and today we're going to talk about the future of Generative AI with LLMs. \n\nAs LLMs continue to improve and become more widely adopted, there are many exciting possibilities for the future of this technology. For example, LLMs could be used to generate personalized news articles or even entire books tailored to an individual's interests and reading level. \n\nWe'll also talk about some of the challenges facing the field of Generative AI, such as the need for more diverse and representative training data, and the potential for LLMs to be used for malicious purposes. And we'll discuss some potential solutions to these challenges, such as the development of more transparent and explainable LLMs. \n\nBy the end of this video, you'll have a better understanding of the current state of the art in Generative AI with LLMs and some ideas for how this technology could evolve in the future. So let's dive in!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-04-05"}}
{"video": {"title": "Building Your Own LLM from Scratch", "transcript": "Hi, I'm Shelbee Eigenbrode, and today we're going to talk about how to build your own LLM from scratch. \n\nBuilding an LLM from scratch can be a challenging but rewarding project. We'll walk through the steps of building an LLM using the transformer architecture, from preparing the data to training the model to evaluating its performance. \n\nWe'll also discuss some best practices for building LLMs, such as using pre-trained models and transfer learning to improve the model's performance, and implementing regularization techniques to prevent overfitting. \n\nBy the end of this video, you'll have a solid understanding of how to build your own LLM from scratch and some practical skills for working with this technology. So let's get started!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-04-12"}}
{"video": {"title": "Advanced Techniques for Training LLMs", "transcript": "Hey there, Mike Chambers here, and today we're going to talk about some advanced techniques for training LLMs. \n\nTraining an LLM can be a complex and computationally intensive process, but there are many techniques that can help improve the model's performance and efficiency. We'll discuss some of these techniques, such as using curriculum learning to gradually increase the difficulty of the training data, and implementing mixed precision training to reduce the memory footprint of the model. \n\nWe'll also talk about some of the challenges of training LLMs, such as the need for large amounts of data and computational resources, and some potential solutions to these challenges, such as using data augmentation and synthetic data generation. \n\nBy the end of this video, you'll have a better understanding of some advanced techniques for training LLMs and some ideas for how to improve the performance of your own models. So let's dive in!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-04-19"}}
{"video": {"title": "Evaluating LLM Performance: Metrics and Best Practices", "transcript": "Hi, I'm Antje Barth, and today we're going to talk about how to evaluate the performance of LLMs. \n\nEvaluating the performance of an LLM can be a complex task, as there are many different metrics and factors to consider. We'll discuss some of the most common metrics used to evaluate LLM performance, such as perplexity and BLEU score, and some best practices for interpreting these metrics. \n\nWe'll also talk about some of the challenges of evaluating LLM performance, such as the need for human evaluation and the potential for biases in the evaluation data. And we'll discuss some potential solutions to these challenges, such as using multiple evaluation metrics and implementing active learning techniques. \n\nBy the end of this video, you'll have a better understanding of how to evaluate the performance of LLMs and some practical skills for working with this technology. So let's get started!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-04-26"}}
{"video": {"title": "Real-World Applications of LLMs", "transcript": "Hey there, Chris Fregly here, and today we're going to talk about some real-world applications of LLMs. \n\nLLMs have a wide range of potential applications, from generating personalized product descriptions for e-commerce sites to creating realistic dialogue for video games. We'll discuss some of these applications in more detail and hear from some AWS AI practitioners who are actively building and deploying LLMs in business use-cases today. \n\nWe'll also talk about some of the challenges of deploying LLMs in the real world, such as the need for scalability and security, and some best practices for addressing these challenges. \n\nBy the end of this video, you'll have a better understanding of some real-world applications of LLMs and some ideas for how to use this technology in your own projects. So let's dive in!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-05-03"}}
{"video": {"title": "The Latest Research in Generative AI with LLMs", "transcript": "Hi, I'm Shelbee Eigenbrode, and today we're going to talk about some of the latest research in Generative AI with LLMs. \n\nThe field of Generative AI is constantly evolving, and there are many exciting developments happening in the world of LLMs. We'll discuss some of the latest research in this area, such as the development of more efficient and interpretable LLMs, and the use of LLMs for tasks like machine translation and summarization. \n\nWe'll also talk about some of the challenges facing the field of Generative AI, such as the need for more diverse and representative training data, and the potential for LLMs to be used for malicious purposes. And we'll discuss some potential solutions to these challenges, such as the development of more transparent and explainable LLMs. \n\nBy the end of this video, you'll have a better understanding of some of the latest research in Generative AI with LLMs and some ideas for how this technology could evolve in the future. So let's get started!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-05-10"}}
{"video": {"title": "Conclusion and Next Steps for Generative AI with LLMs", "transcript": "Hey there, Mike Chambers here, and today we're going to wrap up our video series on Generative AI with LLMs. \n\nWe've covered a lot of ground in this series, from the basics of the transformer architecture to advanced techniques for training and deploying LLMs. We've also discussed some of the ethical considerations surrounding this technology and some real-world applications of LLMs. \n\nBut this is just the beginning of the journey for Generative AI with LLMs. There are many exciting developments happening in this field, and we encourage you to continue exploring and learning about this technology. \n\nWe hope that this video series has given you a solid foundation in Generative AI with LLMs and some practical skills for working with this technology. And we look forward to seeing what you create with LLMs in the future!", "author": "Antje Barth, Chris Fregly, Shelbee Eigenbrode Instructor, Mike Chambers", "publication_date": "2023-05-17"}}
{"video": {"title": "Mastering Diffusion Models: A Hands-On Guide", "transcript": "Hi there, I'm Sharon Zhou, and today we're diving into the fascinating world of diffusion models! \n\nFirst off, what are diffusion models? Well, imagine a system where information spreads over time. That's essentially what diffusion models are all about. They're used in a variety of fields, from physics to social sciences. \n\nNow, let's roll up our sleeves and build our own diffusion model. Fire up your Python environment, and make sure you have Tensorflow or Pytorch installed. We'll start by defining our model architecture, then we'll move on to training our model. \n\nTraining a diffusion model can be time-consuming, but don't worry, I've got a trick up my sleeve. We'll implement an algorithm to speed up sampling by a factor of 10. Yes, you heard it right, 10x faster! \n\nThroughout this video, we'll keep things light and fun. No jargon, I promise. We'll break down complex concepts into simple, digestible bits. And remember, there's no such thing as a silly question. \n\nBy the end of this video, you'll not only understand how diffusion models work, but you'll also have built and trained your own model. Plus, you'll have a nifty algorithm in your toolkit to speed up sampling. \n\nSo, are you ready to embark on this exciting journey? Let's get started! \n\nRemember to hit that like button, share this video with your friends, and don't forget to subscribe for more exciting content. See you in the next one!", "author": "Sharon Zhou", "publication_date": "2022-03-01"}}


{"video": {"title": "Harnessing the Power of AI for Good: A Beginner's Guide", "transcript": "Hi there, I'm Robert Monarch, and today we're diving into the fascinating world of AI, but with a twist. We're not just talking about any AI, we're talking about AI for Good. \n\n(Video hook and introduction) \n\nImagine using artificial intelligence to predict air quality, optimize wind energy, protect biodiversity, or even manage disasters. Sounds like a sci-fi movie, right? Well, it's not. It's happening right now, and you can be a part of it. \n\n(Body content) \n\nFirst, let's understand what AI for Good is. It's a movement, an initiative, a call to action for using AI to tackle some of the world's most pressing issues. From climate change to public health, AI is proving to be a powerful tool. \n\nNow, let's get our hands dirty. We'll walk through a simple framework for developing AI projects. Don't worry, it's beginner-friendly. We'll start with defining the problem, move on to data collection and preparation, then model building, and finally, deployment and monitoring. \n\nThroughout this journey, we'll explore real-world case studies. We'll see how AI is used in public health to predict disease outbreaks, or in climate change to model and predict weather patterns. \n\n(Conclusion and call to action) \n\nBy the end of this series, you'll have a solid understanding of how AI can be used for good. And who knows, you might just be inspired to start your own AI for Good project. So, are you ready to change the world with AI? Let's get started. \n\nRemember, the best way to learn is by doing. So, don't just watch these videos, apply what you learn. And if you have any questions or ideas, share them in the comments. I'm here to help. \n\n", "author": "Robert Monarch", "publication_date": "2022-01-01"}}
{"video": {"title": "Mastering Mathematics for Machine Learning and Data Science: Calculus", "transcript": "Hi there, I'm Luis Serrano, and welcome to our video series on Mathematics for Machine Learning and Data Science. Today, we're diving into the fascinating world of calculus.\n\nCalculus is the study of change, and it's a crucial tool in machine learning. It helps us understand how our models change as the data changes.\n\nLet's start with a simple concept: the derivative. Think of it as the speedometer of a car. It tells us how fast we're going at any given moment.\n\nIn machine learning, the derivative helps us find the best values for our model's parameters. This process is called gradient descent.\n\nNow, let's talk about integration. It's like adding up tiny pieces to find the whole. In machine learning, we use integration to calculate probabilities and expectations.\n\nDon't worry if this seems a bit overwhelming. With practice, these concepts will become second nature.\n\nRemember, every expert was once a beginner. So, keep learning, keep practicing, and soon you'll be a pro at calculus.\n\nThanks for watching. Be sure to like, share, and subscribe for more exciting videos on Mathematics for Machine Learning and Data Science. See you in the next one!\n\n", "author": "Luis Serrano", "publication_date": "2023-03-01"}}
{"video": {"title": "Linear Algebra: The Backbone of Machine Learning", "transcript": "Hey there, I'm Anshuman Singh, and today we're exploring linear algebra, the backbone of machine learning.\n\nLinear algebra is all about vectors and matrices. In machine learning, we use vectors to represent data points and matrices to represent datasets.\n\nLet's talk about matrix multiplication. It's like a super-powered version of regular multiplication. We use it to transform our data in machine learning.\n\nEigenvalues and eigenvectors might sound scary, but they're just special numbers and vectors for a matrix. They're super useful for reducing the dimensions of our data.\n\nDon't worry if this feels a bit challenging. With practice, you'll get the hang of it.\n\nRemember, the journey of a thousand miles begins with a single step. So, keep learning, keep practicing, and soon you'll be a linear algebra whiz.\n\nThanks for watching. Be sure to like, share, and subscribe for more exciting videos on Mathematics for Machine Learning and Data Science. See you next time!\n\n", "author": "Anshuman Singh", "publication_date": "2023-03-03"}}
{"video": {"title": "Demystifying Statistics for Machine Learning", "transcript": "Hello, I'm Elena Sanina, and today we're demystifying statistics for machine learning.\n\nStatistics is the science of data. It helps us make sense of the data we collect and use it to make predictions.\n\nLet's talk about mean, median, and mode. They're simple ways to summarize our data.\n\nStandard deviation and variance help us understand how spread out our data is.\n\nProbability distributions, like the normal distribution, help us predict the likelihood of different outcomes.\n\nDon't worry if this seems a bit complex. With practice, you'll master these concepts in no time.\n\nRemember, the only way to do great work is to love what you do. So, keep learning, keep practicing, and soon you'll be a statistics pro.\n\nThanks for watching. Be sure to like, share, and subscribe for more exciting videos on Mathematics for Machine Learning and Data Science. See you in the next one!\n\n", "author": "Elena Sanina", "publication_date": "2023-03-05"}}
{"video": {"title": "Probability: The Language of Uncertainty in Machine Learning", "transcript": "Hi, I'm Magdalena Bouza, and today we're exploring probability, the language of uncertainty in machine learning.\n\nProbability helps us quantify uncertainty. It's a number between 0 and 1 that tells us how likely an event is to occur.\n\nLet's talk about conditional probability. It's the probability of an event given that another event has occurred.\n\nBayes' theorem helps us update our beliefs based on new data. It's a fundamental concept in machine learning.\n\nDon't worry if this seems a bit tricky. With practice, you'll become fluent in the language of uncertainty.\n\nRemember, the expert in anything was once a beginner. So, keep learning, keep practicing, and soon you'll be a probability pro.\n\nThanks for watching. Be sure to like, share, and subscribe for more exciting videos on Mathematics for Machine Learning and Data Science. See you next time!\n\n", "author": "Magdalena Bouza", "publication_date": "2023-03-07"}}
{"video": {"title": "Calculus in Action: Optimizing Machine Learning Models", "transcript": "Hello, I'm Obed Kobina Nsiah, and today we're seeing calculus in action as we optimize machine learning models.\n\nRemember the derivative from our calculus video? It's a powerful tool for optimization.\n\nIn machine learning, we use the derivative to find the best values for our model's parameters. This process is called gradient descent.\n\nLet's talk about local and global minima. They're the valleys in our optimization landscape.\n\nDon't worry if this seems a bit tough. With practice, you'll be optimizing models like a pro.\n\nRemember, the only limit to your impact is your imagination and commitment. So, keep learning, keep practicing, and soon you'll be an optimization expert.\n\nThanks for watching. Be sure to like, share, and subscribe for more exciting videos on Mathematics for Machine Learning and Data Science. See you in the next one!\n\n", "author": "Obed Kobina Nsiah", "publication_date": "2023-03-09"}}
{"video": {"title": "Linear Algebra in Action: Transforming Data in Machine Learning", "transcript": "Hi, I'm Lucas Coutinho, and today we're seeing linear algebra in action as we transform data in machine learning.\n\nRemember matrices and vectors from our linear algebra video? They're powerful tools for data transformation.\n\nIn machine learning, we use matrices to represent datasets and vectors to represent data points.\n\nLet's talk about matrix multiplication. It's a powerful way to transform our data.\n\nDon't worry if this seems a bit challenging. With practice, you'll be transforming data like a pro.\n\nRemember, the only way to learn mathematics is to do mathematics. So, keep learning, keep practicing, and soon you'll be a data transformation expert.\n\nThanks for watching. Be sure to like, share, and subscribe for more exciting videos on Mathematics for Machine Learning and Data Science. See you next time!\n\n", "author": "Lucas Coutinho", "publication_date": "2023-03-11"}}

{"video": {"title": "Unleashing the Power of NLP with Hugging Face", "transcript": "Hi there, I'm Younes, and today we're diving into the exciting world of Natural Language Processing, or NLP for short. We'll be designing apps that can answer questions, analyze sentiment, translate languages, and even summarize text! \n\nFirst, let's talk about question-answering. Imagine having an app that can answer any question you have about a document. With NLP, that's possible! We'll be using Hugging Face, our technology partner, to make this happen. \n\nNext, we'll explore sentiment analysis. This is where our app can determine if a piece of text is positive, negative, or neutral. It's incredibly useful for analyzing reviews or social media posts. \n\nThen, we'll venture into language translation. With NLP, we can build an app that translates text from one language to another. It's like having your own personal translator! \n\nLastly, we'll look at summarization. Our app will be able to read a long piece of text and provide a short summary. It's perfect for when you don't have time to read a lengthy document. \n\nRemember, NLP is a powerful tool, but it's not perfect. It's always improving, and you can be a part of that improvement. \n\nSo, are you ready to start building? Let's get started! And don't forget to like, share, and subscribe for more exciting content. \n\nUntil next time, keep exploring and innovating.", "author": "Younes Bensouda Mourri, \u0141ukasz Kaiser, Eddy Shyu", "publication_date": "2023-03-15"}}
{"video": {"title": "Mastering Sentiment Analysis with NLP", "transcript": "Hey there, \u0141ukasz here! Today, we're going to master sentiment analysis using NLP. \n\nSentiment analysis is a powerful tool that allows us to understand the emotion behind a piece of text. It's used in marketing, customer service, and even politics. \n\nWe'll be using Hugging Face, our technology partner, to help us build our sentiment analysis app. \n\nFirst, we'll collect our data. This could be reviews, social media posts, or any other text we want to analyze. \n\nNext, we'll preprocess our data. This involves cleaning the text and converting it into a format that our app can understand. \n\nThen, we'll train our model. This is where the magic happens! Our app will learn to recognize the emotion behind the text. \n\nFinally, we'll test our model. We'll see how well it can analyze the sentiment of new pieces of text. \n\nRemember, the more data we have, the better our app will be. So don't be afraid to collect as much data as you can! \n\nSo, are you ready to become a sentiment analysis expert? Let's get started! And don't forget to like, share, and subscribe for more exciting content. \n\nUntil next time, keep exploring and innovating.", "author": "Younes Bensouda Mourri, \u0141ukasz Kaiser, Eddy Shyu", "publication_date": "2023-03-20"}}
{"video": {"title": "Building a Language Translator with NLP", "transcript": "Hello, I'm Eddy, and today we're building a language translator using NLP. \n\nWith NLP, we can build an app that can translate text from one language to another. It's like having your own personal translator! \n\nWe'll be using Hugging Face, our technology partner, to help us build our translator. \n\nFirst, we'll collect our data. This will be text in the language we want to translate from, and the corresponding text in the language we want to translate to. \n\nNext, we'll preprocess our data. This involves cleaning the text and converting it into a format that our app can understand. \n\nThen, we'll train our model. This is where our app learns to translate text from one language to another. \n\nFinally, we'll test our model. We'll see how well it can translate new pieces of text. \n\nRemember, the more data we have, the better our app will be. So don't be afraid to collect as much data as you can! \n\nSo, are you ready to build your own translator? Let's get started! And don't forget to like, share, and subscribe for more exciting content. \n\nUntil next time, keep exploring and innovating.", "author": "Younes Bensouda Mourri, \u0141ukasz Kaiser, Eddy Shyu", "publication_date": "2023-03-25"}}
{"video": {"title": "Mastering Deep Learning Specialization: CNNs, RNNs, LSTMs, and Transformers", "transcript": "Hi there, I'm your host, and today we're diving into the exciting world of deep learning specialization. We'll be building neural networks like CNNs, RNNs, LSTMs, and Transformers, and applying them to speech recognition, NLP, and more, all using Python and TensorFlow. \n\nFirst, let's talk about CNNs, or Convolutional Neural Networks. These are great for image processing tasks, like recognizing faces or objects in photos. We'll walk through how to build one from scratch and then use it to solve a real-world problem. \n\nNext up, we have RNNs, or Recurrent Neural Networks. These are perfect for tasks that involve sequential data, like time series analysis or natural language processing. We'll explore how RNNs work and how to train them effectively. \n\nThen, we'll move on to LSTMs, or Long Short-Term Memory networks. These are a special type of RNN that can handle long-term dependencies in data, making them ideal for tasks like machine translation or speech recognition. \n\nFinally, we'll cover Transformers, the state-of-the-art architecture for natural language processing tasks. We'll see how they work and how to use them to build powerful NLP models. \n\nThroughout the video, we'll be using Python and TensorFlow to build and train our models, so make sure you have those installed before we get started. \n\nNow, let's wrap things up. Deep learning is a powerful tool that can solve a wide range of problems, from image recognition to natural language processing. With the skills you've learned in this video, you'll be able to build and train your own neural networks and apply them to real-world tasks. So what are you waiting for? Get out there and start building! \n\nThanks for watching, and don't forget to like, share, and subscribe for more great content. See you in the next video!", "author": "Andrew Ng, Kian Katanforoosh, Younes Bensouda Mourri", "publication_date": "2023-02-15"}}
